\chapter{Introduction}
\label{chapterlabel1}

\section{Problem Overview}

The volume of information available on the internet have been increasing rapidly with the explosive growth of the World Wide Web, e-Commerce and other online services, making difficult for users to find relevant products in a short period of time. To avoid this problem, many websites use recommendation systems to help customers to find items that satisfies their needs[***](Recommender Systems; Resnick). Suggestions for books on Amazon, or movies on Netflix, are real-world examples of Recommender Systems (RS) that demonstrate that the evolution of (RS) and the web should go hand-in-hand. The final goal of a RS is to generate meaningful recommendations to a collection of users for items that might interest them. 

The design of such recommendation engines depend on the domain and the particular characteristics of the data available. \textit{Collaborative Filtering} systems analyze historical interactions alone, while \textit{Content-based} RS systems are based on user profile attributes; and \textit{hybrid} techniques combine both of these designs. Latest studies in CF [***](Recommender Systems survey) showed that combining conceptual and usage information can improve the quality of web recommendation.

Current RS typically act in a greedy manner by recommending items with the highest user ratings. However, greedy recommendations are suboptimal over the long term. This approach does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the need to explore user preferences and to exploit this information for recommendation.

Therefore, the architecture of recommender systems and their evaluation on real-world problems is an active area of research. In fact, RS have found an active application area for diverse Information Retrieval, Web Mining and Machine Learning techniques. Reinforcement learning[***](Reinforcement Learning: an Introduction) for example, has been suited to solve the recommendation problem. In general, once a user makes her choice based on a list of recommendations given by a RS, a new list of recommended items is then presented. Thus, the recommendation process can be thought as a sequential process as in many domains, user choices are sequential by nature (e.g. a user buy a book by the author of a recent book we liked). 

Indeed, reinforcement learning has been demonstrating to be potentially effective approach in the domain and particularly good to solve the cold-start problem and in some extent, the exploration/exploitation trade-off[***](An MDP-based Recommender System). Nevertheless, it has received relatively little attention and found only limited application.

On the other hand, most recent research, like [***](Collaborative Deep Learning for Recommender Systems), have focused on making a generalization of recent advances in Deep Learning[***](Representation Learning: A Review and New Perspectives) from Identically and Independently Distributed (i.i.d.) input to the non-i.i.d. (CF-based) input, and propose hierarchical Bayesian models that jointly performs deep representation learning for the content information and CF for estimating the ratings matrix. Experiments carried out shown that the Deep Learning approaches can significantly outperform the state of the art. As a result, current research has been getting involved on using deep representational approaches that can lead to better recommendations.

Furthermore, a recent work presented by Dulac-Arnold et al. on deep reinforcement learning in large discrete action spaces[***] presented a new policy architecture that not only allows reinforcement learning methods to be applied to large-scale learning problems, and to operate efficiently with a large number of actions, but also can generalize over the action set in logarithmic time. This algorithm showed to converge under a simulated recommender system environment with a large number of songs.

\section{Dissertation Objectives and Structure}

Chapter 2 introduces some definitions of Recommender Systems and Reinforcement Learning, Later, it reviews some related work that has been made for modeling recommender systems using reinforcement learning approaches and deep learning. Chapter 3 defines the proposed model and the implementation details, while Chapter 4 details the information about experiments carried out and evaluation of results. Finally, we present a summary of lessons learned, conclusions and future work of the thesis project.


%Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.
%\blindtext
