\chapter{Introduction}
\label{chapterlabel1}

The volume of information available on the internet have been increasing rapidly with the explosive growth of the World Wide Web, e-Commerce and other online services, making difficult for users to find relevant products in a short period of time. To avoid this problem, many websites use recommendation systems to help customers to find items that satisfy their needs \cite{resnick1997recommender}. Suggestions for books on Amazon, or movies on Netflix, are real-world examples of Recommender Systems (RS) that have demonstrated that the evolution of RS and the web should go hand-in-hand to achieve the common goal of efficiently managing an ever growing set of items and  thus generate meaningful recommendations to a collection of users with common interests.

\section{Problem Overview}

The design of such recommendation engines depends on the domain and the particular characteristics of the data available. While \textit{Collaborative Filtering} systems analyze historical interactions alone, \textit{Content-based} RS systems are based on user profile attributes; and finally, \textit{hybrid} techniques combine both of these designs. Latest studies in CF \cite{bobadilla2013recommender} showed that combining conceptual and usage information can improve the quality of web recommendation.

Current RS typically act in a \textit{greedy} manner by recommending items with the highest user ratings. However, greedy recommendations are suboptimal over the long term. This approach does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the need to explore user preferences and to exploit this information for recommendation.

Therefore, the architecture of recommender systems and their performance on real-world problems is an active area of research. where approaches using a diverse Information Retrieval, Web Mining and Machine Learning techniques have been proposed \cite{bobadilla2013recommender}. Reinforcement learning\cite{sutton1998reinforcement} for example, can be used to try to solve the recommendation task. 

In general, once a user selects a in item based on a list of recommendations given by a RS, a new list of recommended items is then presented. This recommendation process can be then thought as a sequential process, where users choices are sequential by nature (e.g. a user buy a book by the author of a recent book she liked).

Indeed, reinforcement learning has demonstrated in the past to be a potentially effective approach in the domain and particularly good to solve the cold-start problem and in some extent, the exploration/exploitation trade-off \cite{shani2005mdp}. Nevertheless, it has received relatively little attention and found only limited application as its computational complexity made the problem intractable when there is a high number of items to recommend.

On the other hand, most recent research \cite{wang2015collaborative} have focused on making a generalization of recent advances in Deep Learning \cite{bengio2013representation} from Identically and Independently Distributed (i.i.d.) input to the non-i.i.d. (CF-based) input, and propose hierarchical Bayesian models that jointly performs deep representation learning for the content information and CF for estimating the ratings matrix. Experiments carried out shown that Deep Learning approaches can significantly outperform the state of the art. As a result, current research has been getting involved on using deep representational approaches that can lead to better recommendations.

Furthermore, a recent work presented by Dulac-Arnold et al. \cite{Dulac-Arnold2015} use deep neural neural networks and reinforcement learning in large discrete action spaces  and propose a new policy architecture that not only allows RL methods to be applied to large-scale learning problems, and to operate efficiently with a large number of actions, but also is able to generalize over the action set in logarithmic time. This algorithm showed to a promising convergence of the model among different continuous environments.

\section{Dissertation Objectives and Contributions}

The main objective of this project is to show that a deep reinforcement learning model is able to perform well under the RS domain by learning how to generate good recommendations to users over a large set of items. After defining the recommendation problem under the reinforcement learning domain, we describe a baseline model using the Deep Deterministic Policy Gradient algorithm with a k-nearest neighbours (kNN) policy for action space reduction, and trained it using a content-based and a collaborative-filtering representation of items. Additionally, we propose a new policy architecture based on a Factorization Machine (FM) model for ranking recommendation, and compare the performance of the reinforcement agent under different policy definitions and network configurations.

Then, for each model we evaluate the average return obtained by the learning agent, as well as the average precision along user sessions or episodes of learning. Finally, a quantitative and qualitative analysis of the results shows that both the FM-based policy and the kNN-based policy architectures are able to generate good recommendations, however, the former outperforms the baseline models and presents an slightly better progression on its performance over the learning steps.

\section{Dissertation Objectives and Structure}

The remaining chapters of this report are organized as follows. Chapter \ref{sec:chapterlabel2} defines the recommender problem and introduces Reinforcement Learning. Following, it reviews some related work that has been made for modeling recommender systems using reinforcement learning approaches and deep learning. The proposed model and its implementation details are presented in Chapter \ref{sec:chapterlabel3}, while the experimental evaluation and results are analyzed in Chapter \ref{sec:chapterlabel4}. Finally, we present a summary of lessons learned and conclusions, and propose some ideas for future work.